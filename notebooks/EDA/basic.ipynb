{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(r\"F:\\machine learning\\mlops\\end to end machine learning pipeline\\MLOPs_workflow\\data\\raw\\AB_NYC_2019.csv\")\n",
    "\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>host_id</th>\n",
       "      <th>host_name</th>\n",
       "      <th>neighbourhood_group</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>room_type</th>\n",
       "      <th>price</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>last_review</th>\n",
       "      <th>reviews_per_month</th>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <th>availability_365</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2539</td>\n",
       "      <td>Clean &amp; quiet apt home by the park</td>\n",
       "      <td>2787</td>\n",
       "      <td>John</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Kensington</td>\n",
       "      <td>40.64749</td>\n",
       "      <td>-73.97237</td>\n",
       "      <td>Private room</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2018-10-19</td>\n",
       "      <td>0.21</td>\n",
       "      <td>6</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2595</td>\n",
       "      <td>Skylit Midtown Castle</td>\n",
       "      <td>2845</td>\n",
       "      <td>Jennifer</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Midtown</td>\n",
       "      <td>40.75362</td>\n",
       "      <td>-73.98377</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>2019-05-21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2</td>\n",
       "      <td>355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                name  host_id host_name  \\\n",
       "0  2539  Clean & quiet apt home by the park     2787      John   \n",
       "1  2595               Skylit Midtown Castle     2845  Jennifer   \n",
       "\n",
       "  neighbourhood_group neighbourhood  latitude  longitude        room_type  \\\n",
       "0            Brooklyn    Kensington  40.64749  -73.97237     Private room   \n",
       "1           Manhattan       Midtown  40.75362  -73.98377  Entire home/apt   \n",
       "\n",
       "   price  minimum_nights  number_of_reviews last_review  reviews_per_month  \\\n",
       "0    149               1                  9  2018-10-19               0.21   \n",
       "1    225               1                 45  2019-05-21               0.38   \n",
       "\n",
       "   calculated_host_listings_count  availability_365  \n",
       "0                               6               365  \n",
       "1                               2               355  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48895 entries, 0 to 48894\n",
      "Data columns (total 16 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   id                              48895 non-null  int64  \n",
      " 1   name                            48879 non-null  object \n",
      " 2   host_id                         48895 non-null  int64  \n",
      " 3   host_name                       48874 non-null  object \n",
      " 4   neighbourhood_group             48895 non-null  object \n",
      " 5   neighbourhood                   48895 non-null  object \n",
      " 6   latitude                        48895 non-null  float64\n",
      " 7   longitude                       48895 non-null  float64\n",
      " 8   room_type                       48895 non-null  object \n",
      " 9   price                           48895 non-null  int64  \n",
      " 10  minimum_nights                  48895 non-null  int64  \n",
      " 11  number_of_reviews               48895 non-null  int64  \n",
      " 12  last_review                     38843 non-null  object \n",
      " 13  reviews_per_month               38843 non-null  float64\n",
      " 14  calculated_host_listings_count  48895 non-null  int64  \n",
      " 15  availability_365                48895 non-null  int64  \n",
      "dtypes: float64(3), int64(7), object(6)\n",
      "memory usage: 6.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "def delta_date_feature(dates):\n",
    "    \"\"\"\n",
    "    Given a 2D array containing dates, returns the delta in days between each date \n",
    "    and the most recent date in its column.\n",
    "    \"\"\"\n",
    "    dates = pd.DataFrame(x, columns=[\"last_review\"])\n",
    "    dates['last_review'] = pd.to_datetime(dates[\"last_review\"], format=f\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "    max_dates = dates['last_review'].max()\n",
    "    return dates['last_review'].apply(lambda d : (max_dates - d)).dt.days.fillna(max_dates).to_numpy().reshape(-1, 1)\n",
    "\n",
    "\n",
    "def get_feature_transformation_pipeline():\n",
    "    \"\"\"\n",
    "    Constructs a feature transformation pipeline.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    feature_transformation_pipeline : Pipeline\n",
    "        Scikit-learn pipeline for feature preprocessing.\n",
    "    processed_features : list\n",
    "        List of input features before transformation.\n",
    "    new_features : list\n",
    "        List of transformed feature names.\n",
    "    \"\"\"\n",
    "\n",
    "    # Categorical Features\n",
    "    # ordinal_categorical = [\"room_type\"]\n",
    "    non_ordinal_categorical = [\"neighbourhood_group\"]\n",
    "\n",
    "    ordinal_categorical_preproc = OrdinalEncoder()\n",
    "\n",
    "    non_ordinal_categorical_preproc = make_pipeline(\n",
    "        SimpleImputer(strategy=\"most_frequent\"),\n",
    "        OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    )\n",
    "\n",
    "    ordinal_categorical = [\"room_type\"]\n",
    "    non_ordinal_categorical = [\"neighbourhood_group\"]\n",
    "    # Numerical Features with Zero Imputation\n",
    "    zero_imputed_columns = [\n",
    "        \"minimum_nights\",\n",
    "        \"number_of_reviews\",\n",
    "        \"reviews_per_month\",\n",
    "        \"calculated_host_listings_count\",\n",
    "        \"availability_365\",\n",
    "        \"longitude\",\n",
    "        \"latitude\"\n",
    "    ]\n",
    "    zero_imputer = SimpleImputer(strategy=\"constant\", fill_value=0)\n",
    "\n",
    "    # Date Transformation\n",
    "    date_imputer = make_pipeline(\n",
    "        SimpleImputer(strategy='constant', fill_value='2010-01-01'),\n",
    "        FunctionTransformer(delta_date_feature, validate=False)\n",
    "    )\n",
    "\n",
    "    # Text Feature Engineering for 'name' column\n",
    "    name_tfidf = make_pipeline(\n",
    "        SimpleImputer(strategy=\"constant\", fill_value=\"\"),\n",
    "        FunctionTransformer(lambda x: x.ravel(), validate=False),  # Ensures 1D input for TF-IDF\n",
    "        TfidfVectorizer(binary=False, max_features=5, stop_words='english')\n",
    "    )\n",
    "\n",
    "    # Column Transformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            # (\"ordinal_cat\", ordinal_categorical_preproc, ordinal_categorical),\n",
    "            (\"non_ordinal_cat\", non_ordinal_categorical_preproc, non_ordinal_categorical),\n",
    "            (\"impute_zero\", zero_imputer, zero_imputed_columns),\n",
    "            (\"transform_date\", date_imputer, [\"last_review\"]),\n",
    "            (\"transform_name\", name_tfidf, [\"name\"])\n",
    "        ],\n",
    "        remainder=\"drop\"  # Drops unused columns\n",
    "    )\n",
    "\n",
    "    # Feature Lists\n",
    "    processed_features = ordinal_categorical + non_ordinal_categorical + zero_imputed_columns + [\"last_review\", \"name\"]\n",
    "\n",
    "    new_features = ordinal_categorical + \\\n",
    "                   ['neighbourhood_group_Bronx', 'neighbourhood_group_Brooklyn', \n",
    "                    'neighbourhood_group_Manhattan', 'neighbourhood_group_Queens', \n",
    "                    'neighbourhood_group_Staten Island'] + \\\n",
    "                   zero_imputed_columns + ['last_review'] + \\\n",
    "                   ['apartment', 'bedroom', 'cozy', 'private', 'room']\n",
    "\n",
    "    # Final Pipeline\n",
    "    # feature_transformation_pipeline = Pipeline(\n",
    "    #     steps=[(\"preprocessor\", preprocessor)]\n",
    "    # )\n",
    "\n",
    "    return feature_transformation_pipeline, processed_features, new_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline, processed_features, new_features = get_feature_transformation_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = pipeline.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step_name, step in pipeline.named_steps.items():\n",
    "    print(f\"üîç Checking step: {step_name}\")\n",
    "    try:\n",
    "        transformed = step.fit_transform(data)\n",
    "        print(f\" Step '{step_name}' completed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error in step '{step_name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "parquet_file_path = Path(\"F://machine learning//mlops//end to end machine learning pipeline//MLOPs_workflow//data//processed//target.parquet\")\n",
    "\n",
    "data = pd.read_parquet(parquet_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price', 'id', 'event_timestamp'], dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'room_type', 'neighbourhood_group_Bronx',\n",
    "       'neighbourhood_group_Brooklyn', 'neighbourhood_group_Manhattan',\n",
    "       'neighbourhood_group_Queens', 'id', 'event_timestamp']\n",
    "\n",
    "\n",
    "'neighbourhood_group_Staten Island', 'minimum_nights',\n",
    "       'number_of_reviews', 'reviews_per_month', 'id', 'event_timestamp'\n",
    "\n",
    "\n",
    "'calculated_host_listings_count', 'availability_365', 'longitude',\n",
    "       'latitude', 'id', 'event_timestamp'\n",
    "\n",
    "\n",
    "'last_review', 'apartment', 'bedroom', 'cozy', 'private', 'room', 'id',\n",
    "       'event_timestamp'\n",
    "\n",
    "\n",
    "'price', 'id', 'event_timestamp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step1: room type ko OrdinalEncoder se encode karo \n",
    "\n",
    "# step2: non ordinal category ke liye phele most frequent fill karo then one hot encode karo\n",
    "\n",
    "# step3: zero impute karo using SimpleImputer wth constant value \n",
    "\n",
    "# step4: date transformation karo using SimpleImputer with constant value and then FunctionTransformer)\n",
    "\n",
    "# step5: name column ke liye phele most frequent fill karo then TfidfVectorizer use karo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]]\n",
      "file saved\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "\n",
    "class OrdinalEncoderTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Applies Ordinal Encoding to a specified column and persists the fitted encoder.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the transformer with a specific column to encode.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        column_name : str\n",
    "            The name of the column to apply ordinal encoding.\n",
    "        \"\"\"\n",
    "        self.encoder = OrdinalEncoder()\n",
    "\n",
    "    def fit(self, X, column_name):\n",
    "        \"\"\"\n",
    "        Fits the ordinal encoder on the specified column.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Input dataframe containing the column to be encoded.\n",
    "        \"\"\"\n",
    "        if column_name not in X:\n",
    "            raise ValueError(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "        \n",
    "        self.encoder.fit(X[[column_name]])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, column_name):\n",
    "        \"\"\"\n",
    "        Transforms the specified column using the fitted encoder.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame\n",
    "            Input dataframe containing the column to be transformed.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray\n",
    "            Transformed column as a NumPy array.\n",
    "        \"\"\"\n",
    "        if column_name not in X:\n",
    "            raise ValueError(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "        \n",
    "        # no extra column will be created here as we are transforming only one column\n",
    "        return self.encoder.transform(X[[column_name]])\n",
    "\n",
    "    def fit_transform(self, X, column_name):\n",
    "        \"\"\"Fits and transforms the data in one step.\"\"\"\n",
    "        return self.fit(X, column_name).transform(X, column_name)\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Saves the fitted encoder to a file.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        path : str\n",
    "            The file path to save the encoder.\n",
    "        \"\"\"\n",
    "        artifact_path = path / \"ordinal_encoder.pkl\"\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "      \n",
    "        with open(artifact_path, \"wb\") as f:\n",
    "            pickle.dump(self.encoder, f)\n",
    "            print(\"file saved\")\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        \"\"\"\n",
    "        Loads a previously saved encoder.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        path : str\n",
    "            The file path from where to load the encoder.\n",
    "        column_name : str\n",
    "            The column name for which the encoder was originally created.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        OrdinalEncoderTransformer\n",
    "            A new instance of OrdinalEncoderTransformer with the loaded encoder.\n",
    "\n",
    "\n",
    "\n",
    "        usage : \n",
    "            # Load the trained encoder\n",
    "            loaded_transformer = OrdinalEncoderTransformer.load(\"ordinal_encoder.pkl\", column_name=\"room_type\")\n",
    "        \"\"\"\n",
    "\n",
    "        artifact_path = path.joinpath(\"ordinal_encoder.pkl\")\n",
    "        with open(artifact_path, \"rb\") as f:\n",
    "            loaded_encoder = pickle.load(f)\n",
    "        \n",
    "        # create a new instance with the column name and loaded encoder\n",
    "        # return the new instance to the caller\n",
    "        transformer = OrdinalEncoderTransformer()\n",
    "        transformer.encoder = loaded_encoder\n",
    "        return transformer\n",
    "    \n",
    "\n",
    "# usage :\n",
    "\n",
    "# Initialize and fit transformer\n",
    "ordinal_transformer = OrdinalEncoderTransformer()\n",
    "ordinal_transformer.fit(data, column_name=\"room_type\")\n",
    "\n",
    "# Transform data\n",
    "transformed_data = ordinal_transformer.transform(data, column_name=\"room_type\")\n",
    "print(transformed_data)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "ordinal_transformer.save(Path(\"room_type\"))\n",
    "loaded_transformer = OrdinalEncoderTransformer.load(Path(\"room_type\"))\n",
    "x = loaded_transformer.transform(data, column_name=\"room_type\")\n",
    "# print(transformed_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>room_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48890</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48891</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48892</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48893</th>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48894</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48895 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       room_type\n",
       "0            1.0\n",
       "1            0.0\n",
       "2            1.0\n",
       "3            0.0\n",
       "4            0.0\n",
       "...          ...\n",
       "48890        1.0\n",
       "48891        1.0\n",
       "48892        0.0\n",
       "48893        2.0\n",
       "48894        1.0\n",
       "\n",
       "[48895 rows x 1 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x, columns = ['room_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48895, 16)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£ BaseEstimator\n",
    "# ‚úî Provides basic methods like get_params() and set_params()\n",
    "# ‚úî Allows the transformer to be used in hyperparameter tuning (GridSearchCV, RandomizedSearchCV)\n",
    "# ‚úî Helps maintain a consistent API across Scikit-Learn components\n",
    "\n",
    "# üîπ Example Usage of get_params()\n",
    "\n",
    "# 2Ô∏è‚É£ TransformerMixin\n",
    "# ‚úî Ensures that the transformer supports fit_transform(X, y=None) method\n",
    "# ‚úî Reduces redundant code by automatically defining fit_transform() as fit().transform(X)\n",
    "# ‚úî Makes the transformer pipeline-compatible\n",
    "\n",
    "# Without TransformerMixin, we would need to manually implement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Categorical Features\n",
    "# # ordinal_categorical = [\"room_type\"]\n",
    "# non_ordinal_categorical = [\"neighbourhood_group\"]\n",
    "\n",
    "# # ordinal_categorical_preproc = OrdinalEncoder()\n",
    "\n",
    "# non_ordinal_categorical_preproc = make_pipeline(\n",
    "#     SimpleImputer(strategy=\"most_frequent\"),\n",
    "#     OneHotEncoder(handle_unknown=\"ignore\")\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neighborhood_Bronx', 'neighborhood_Brooklyn',\n",
       "       'neighborhood_Manhattan', 'neighborhood_Queens',\n",
       "       'neighborhood_Staten Island'], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class NonOrdinalCategoricalTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer for handling non-ordinal categorical features.\n",
    "    1. Imputes missing values using the most frequent category.\n",
    "    2. Applies One-Hot Encoding with `handle_unknown=\"ignore\"` to avoid errors during inference.\n",
    "\n",
    "    Attributes:\n",
    "    ------------\n",
    "    imputer : SimpleImputer\n",
    "        Imputer for handling missing categorical values.\n",
    "    encoder : OneHotEncoder\n",
    "        One-hot encoder for transforming categorical values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "        self.encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "    def fit(self, X, column_name):\n",
    "        \"\"\"\n",
    "        Fits the imputer and the one-hot encoder.\n",
    "        \"\"\"\n",
    "        out = self.imputer.fit_transform(X[column_name].values.reshape(-1, 1))  # Fit imputer\n",
    "        # print(X[column_name])\n",
    "        self.encoder.fit(out)  # Fit encoder on imputed data\n",
    "        return self  # Return self for method chaining\n",
    "\n",
    "    def transform(self, X, column_name):\n",
    "        \"\"\"\n",
    "        Applies imputation and one-hot encoding to the data.\n",
    "        \"\"\"\n",
    "        out = self.imputer.transform(X[column_name].values.reshape(-1, 1))# Impute missing values\n",
    "        X_encoded = self.encoder.transform(out)  # One-hot encode\n",
    "        # print(X_encoded.shape)\n",
    "\n",
    "        return pd.DataFrame(X_encoded, columns=[self.get_feature_names(column_name)])\n",
    "        \n",
    "\n",
    "    def fit_transform(self, X, column_name):\n",
    "        \"\"\"\n",
    "        Combines fit and transform for efficiency.\n",
    "        \"\"\"\n",
    "        return self.fit(X, column_name).transform(X, column_name)\n",
    "\n",
    "    def get_feature_names(self, column_name):\n",
    "        \"\"\"\n",
    "        Returns the feature names for the encoded categories.\n",
    "        \"\"\"\n",
    "        return self.encoder.get_feature_names_out([column_name])\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Saves the transformer (including imputer and encoder) as a pickle file.\n",
    "        \"\"\"\n",
    "\n",
    "        # save the imputer first and then the encode\n",
    "\n",
    "        imputer_path = path / \"imputer.pkl\"\n",
    "        encoder_path = path / \"encoder.pkl\"\n",
    "\n",
    "\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with open(imputer_path, \"wb\") as f:\n",
    "            pickle.dump(self.imputer, f)\n",
    "\n",
    "        with open(encoder_path, \"wb\") as f:\n",
    "            pickle.dump(self.encoder, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        \"\"\"\n",
    "        Loads a saved transformer from a pickle file.\n",
    "        \"\"\"\n",
    "\n",
    "        # from the current class's artifact folder get the artifacts \n",
    "        for artifact in path.iterdir():\n",
    "            if \"imputer\" in str(artifact):\n",
    "                with open(artifact, \"rb\") as f:\n",
    "                    imputer_pkl = pickle.load(f)\n",
    "            else:\n",
    "                with open(artifact, \"rb\") as f:\n",
    "                    encoder_pkl = pickle.load(f)\n",
    "            \n",
    "\n",
    "        \n",
    "        # create a new instance with the loaded pickle file \n",
    "        non_ordinal_categorical_transformer = NonOrdinalCategoricalTransformer()\n",
    "        non_ordinal_categorical_transformer.imputer = imputer_pkl\n",
    "        non_ordinal_categorical_transformer.encoder = encoder_pkl\n",
    "\n",
    "        # return the instance of this class with arguments already loaded\n",
    "        # this can be directly used for transformation\n",
    "        # example provided below \n",
    "        return non_ordinal_categorical_transformer\n",
    "\n",
    "\n",
    "NonOrdinalCategoricalTransformer_obj = NonOrdinalCategoricalTransformer()\n",
    "NonOrdinalCategoricalTransformer_obj.fit(data, column_name=\"neighbourhood_group\")\n",
    "transformed_data_nonordinal = NonOrdinalCategoricalTransformer_obj.transform(data, column_name=\"neighbourhood_group\")\n",
    "\n",
    "NonOrdinalCategoricalTransformer_obj.save(Path(\"neighborhood\"))\n",
    "\n",
    "\n",
    "# this instance will be loaded with \n",
    "non_ordinal_categorical_transformer = NonOrdinalCategoricalTransformer.load(Path(\"neighborhood\"))\n",
    "non_ordinal_categorical_transformer.encoder.get_feature_names_out(['neighborhood'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Features with Zero Imputation\n",
    "mean_imputed_columns = [\n",
    "    \"minimum_nights\",\n",
    "    \"number_of_reviews\",\n",
    "    \"reviews_per_month\",\n",
    "    \"calculated_host_listings_count\",\n",
    "    \"availability_365\",\n",
    "    \"longitude\",\n",
    "    \"latitude\"\n",
    "]\n",
    "mean_imputer = SimpleImputer(strategy=\"mean\", fill_value=0)\n",
    "mean_imputed = mean_imputer.fit_transform(data[mean_imputed_columns])\n",
    "\n",
    "mean_imputed_df = pd.DataFrame(mean_imputed, columns=mean_imputed_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date Transformation\n",
    "# date_imputer = make_pipeline(\n",
    "#     SimpleImputer(strategy='constant', fill_value='2010-01-01'),\n",
    "#     FunctionTransformer(delta_date_feature, validate=False)\n",
    "# )\n",
    "\n",
    "# date_imputer.fit_transform(data[[\"last_review\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_copy = data.copy()\n",
    "# imputer = SimpleImputer(strategy='constant', fill_value='2010-01-01')\n",
    "# # first fill null values\n",
    "# data_copy['last_review_date'] = imputer.fit_transform(data_copy['last_review'].values.reshape(-1, 1)).reshape(-1,)\n",
    "\n",
    "# # then convert to pd datetime format\n",
    "# data_copy['last_review_date']  = pd.to_datetime(data_copy['last_review_date'])\n",
    "\n",
    "# # then get the max value\n",
    "# np.max(data_copy['last_review_date'])\n",
    "\n",
    "\n",
    "# (np.max(data_copy['last_review_date']) - data_copy['last_review_date']).dt.days.fillna(0).values\n",
    "# imputer.fit_transform(data_copy['last_review'].values.reshape(-1, 1)).reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src\\project\\prod\\prod_artifacts\\DateFeature\\imputer.pkl\n",
      "src\\project\\prod\\prod_artifacts\\DateFeature\\max_value.pkl\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class DeltaDatetimeFeature(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        \"\"\" we need a imputer to fill null values and a function transformer to transform the date\"\"\"\n",
    "        self.imputer = SimpleImputer(strategy='constant', fill_value='2010-01-01')\n",
    "        self.max_date = None\n",
    "        \n",
    "        \n",
    "\n",
    "    def fit(self, X, column_name):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        # Convert column to string before applying SimpleImputer\n",
    "        X_copy[column_name] = X_copy[column_name].astype(str)\n",
    "        \n",
    "        # Impute missing values with \"2010-01-01\"\n",
    "        X_copy[column_name] = self.imputer.fit_transform(X_copy[[column_name]]).ravel()\n",
    "\n",
    "        # Convert back to datetime\n",
    "        X_copy[column_name] = pd.to_datetime(X_copy[column_name], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "        # Compute max date while handling NaT values\n",
    "        self.max_date = np.max(X_copy[column_name])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, column_name):\n",
    "        # now caluclate the delta date\n",
    "        data_copy = X.copy()\n",
    "        # use the imputer to fill null values\n",
    "        data_copy['last_review_date'] = imputer.fit_transform(data_copy['last_review'].values.reshape(-1, 1)).reshape(-1,)\n",
    "\n",
    "        # Convert to datetime, handling errors gracefully\n",
    "        data_copy['last_review_date'] = pd.to_datetime(data_copy['last_review_date'], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "        if pd.isna(self.max_date):  # If all values are NaT, return a default fill value\n",
    "            return np.full((len(data_copy), 1), fill_value=-1)  # -1 can indicate missing values\n",
    "\n",
    "        # Vectorized computation of delta in days\n",
    "        delta_days = (self.max_date - data_copy['last_review_date']).dt.days.fillna(0).values\n",
    "        return pd.DataFrame(delta_days,columns=[\"days_from_max_date\"])    \n",
    "    \n",
    "    def fit_transform(self, X, column_name):\n",
    "        return self.fit(X, column_name).transform(X, column_name)\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\" save the artifacts of the model\"\"\"\n",
    "\n",
    "        imputer_path = path / \"imputer.pkl\"\n",
    "        max_date_val = path / \"max_value.pkl\"\n",
    "\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with open(imputer_path, \"wb\") as f:\n",
    "            pickle.dump(self.imputer, f)\n",
    "        \n",
    "        with open(max_date_val, \"wb\") as f:\n",
    "            pickle.dump(self.max_date, f)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        \n",
    "        for artifact in path.iterdir():\n",
    "            print(artifact)\n",
    "            if \"imputer\" in str(artifact):\n",
    "                with open(artifact, \"rb\") as f:\n",
    "                    imputer = pickle.load(f)\n",
    "            else:\n",
    "                with open(artifact, \"rb\") as f:\n",
    "                    max_value = pickle.load(f)\n",
    "        \n",
    "        # create a new instance of the class\n",
    "        delta_date_feature = DeltaDatetimeFeature()\n",
    "        delta_date_feature.imputer = imputer   \n",
    "        delta_date_feature.max_date = max_value\n",
    "\n",
    "        return delta_date_feature\n",
    "\n",
    "\n",
    "\n",
    "deltadatetimefeature_obj = DeltaDatetimeFeature()\n",
    "deltadatetimefeature_obj.fit(data, 'last_review')\n",
    "out = deltadatetimefeature_obj.transform(data, 'last_review')\n",
    "\n",
    "deltadatetimefeature_obj.save(Path(\"src\\project\\prod\\prod_artifacts\\DateFeature\"))\n",
    "new_obj = DeltaDatetimeFeature.load(Path(\"src\\project\\prod\\prod_artifacts\\DateFeature\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tfidfVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.imputer = SimpleImputer(strategy=\"constant\", fill_value=\"\")\n",
    "        self.vectorizer = TfidfVectorizer(binary=False, max_features=5, stop_words='english')\n",
    "\n",
    "    def fit(self, X, column_name):\n",
    "        new = self.imputer.fit_transform([X[column_name]])        # or [[]] --> .reshape(-1, 1)\n",
    "        self.vectorizer.fit(new.ravel())\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, column_name):\n",
    "        data_copy = X.copy()\n",
    "        new = self.imputer.transform([data_copy[column_name]])\n",
    "        print(new.shape)\n",
    "        vectorized = self.vectorizer.transform(new.ravel())\n",
    "        \n",
    "        # vectorized are csr_matrix, we need to convert it into a dataframe \n",
    "        return pd.DataFrame(vectorized.toarray(), columns=self.get_feature_names())\n",
    "    \n",
    "    def get_feature_names(self,):\n",
    "        return self.vectorizer.get_feature_names_out().tolist()\n",
    "    \n",
    "    def fit_transform(self, X, column_name):\n",
    "        return self.fit(X, column_name).transform(X, column_name)\n",
    "\n",
    "    def save(self, path):\n",
    "        imputer_path = path / \"imputer.pkl\"\n",
    "        vectorizer_path = path / \"vectorizer.pkl\"\n",
    "\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(imputer_path, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "        \n",
    "        with open(vectorizer_path, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        for artifact in path.iterdir():\n",
    "            print(artifact)\n",
    "            if \"imputer\" in str(artifact):\n",
    "                with open(artifact, \"rb\") as f:\n",
    "                    imputer = pickle.load(f)\n",
    "            else:\n",
    "                with open(artifact, \"rb\") as f:\n",
    "                    vectorizer = pickle.load(f)\n",
    "        \n",
    "        tfidfVectorizerobj = tfidfVectorizer()\n",
    "        tfidfVectorizerobj.imputer = imputer\n",
    "        tfidfVectorizerobj.vectorizer = vectorizer\n",
    "\n",
    "        return tfidfVectorizerobj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 48895)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apartment</th>\n",
       "      <th>bedroom</th>\n",
       "      <th>cozy</th>\n",
       "      <th>private</th>\n",
       "      <th>room</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48890</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48891</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48892</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48893</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48894</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48895 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       apartment  bedroom  cozy  private  room\n",
       "0            0.0      0.0   0.0      0.0   0.0\n",
       "1            0.0      0.0   0.0      0.0   0.0\n",
       "2            0.0      0.0   0.0      0.0   0.0\n",
       "3            0.0      0.0   1.0      0.0   0.0\n",
       "4            0.0      0.0   0.0      0.0   0.0\n",
       "...          ...      ...   ...      ...   ...\n",
       "48890        0.0      1.0   0.0      0.0   0.0\n",
       "48891        0.0      0.0   0.0      0.0   1.0\n",
       "48892        0.0      0.0   0.0      0.0   0.0\n",
       "48893        0.0      0.0   1.0      0.0   0.0\n",
       "48894        0.0      0.0   0.0      0.0   0.0\n",
       "\n",
       "[48895 rows x 5 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidfVectorizerobj = tfidfVectorizer()\n",
    "# tfidfVectorizerobj.fit(data, \"name\")\n",
    "# tfidfVectorizerobj.transform(data, 'name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the full pipeline output \n",
    "\n",
    "# har ek class se khali load karo artifacts and transform karo data\n",
    "# original dataframe se kaunse kaunse features nahi hai transformed mein wo add karke dekhna hai \n",
    "# ek new dataframe banao having transformed dataset\n",
    "# \n",
    "def MeanImputer(columns):\n",
    "    \"\"\" \n",
    "    impute null values with mean values , doest word column wise but will work on overall data \n",
    "    \n",
    "    \"\"\"\n",
    "    # Numerical Features with Zero Imputation\n",
    "    mean_imputer = SimpleImputer(strategy=\"mean\", fill_value=0)\n",
    "    mean_imputed = mean_imputer.fit_transform(data[columns])\n",
    "\n",
    "    mean_imputed_df = pd.DataFrame(mean_imputed, columns=columns)\n",
    "    return mean_imputed_df\n",
    "\n",
    "\n",
    "def pipeline(df, path, training=True):\n",
    "    \"\"\"\n",
    "\n",
    "    will take a dataframe as input and process the required features and return the transformed dataframe,\n",
    "    if the training args is true that means we need to train the sklearn artifacts used in each classes and save them in respectve \n",
    "    folders\n",
    "\n",
    "    Parameters:\n",
    "    ---------------------\n",
    "    df : pd.DataFrame\n",
    "        dataframe to preprocess\n",
    "    \n",
    "    path : pathlib.Path\n",
    "        path to store or retrieve artifacts\n",
    "    \n",
    "    training : boolean\n",
    "        whether to fit and save the artifacts or retrieve the fitter artifacts\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # get the columns that need processing\n",
    "    ordinal_categorical = [\"room_type\"]\n",
    "    non_ordinal_categorical = [\"neighbourhood_group\"]\n",
    "        \n",
    "    # Numerical Features with Zero Imputation\n",
    "    mean_imputed_columns = [\n",
    "        \"minimum_nights\",\n",
    "        \"number_of_reviews\",\n",
    "        \"reviews_per_month\",\n",
    "        \"calculated_host_listings_count\",\n",
    "        \"availability_365\",\n",
    "        \"longitude\",\n",
    "        \"latitude\"\n",
    "        ]\n",
    "    \n",
    "    \n",
    "    \n",
    "    if training:\n",
    "        # we will only save the artifacts in the respective folder\n",
    "\n",
    "        # first for the given dataframe, process the ordinal categorial features \n",
    "        oheobj = OrdinalEncoderTransformer()\n",
    "        oheobj.fit(df, column_name=ordinal_categorical[0])\n",
    "        oheobj.save(path / \"OrdinalCat\")\n",
    "\n",
    "        nonordinalprocess = NonOrdinalCategoricalTransformer()\n",
    "        nonordinalprocess.fit(df, column_name=non_ordinal_categorical[0])\n",
    "        nonordinalprocess.save(path / \"NonOrdinalCat\")\n",
    "\n",
    "        datefeatureprocess = DeltaDatetimeFeature()\n",
    "        datefeatureprocess.fit(df, column_name=\"last_review\")\n",
    "        datefeatureprocess.save(path / \"DateFeature\")\n",
    "\n",
    "        # training doesnt need mean imputation\n",
    "        # meanimputerprocessed = MeanImputer(mean_imputed_columns)\n",
    "\n",
    "        nameprocess = tfidfVectorizer()\n",
    "        nameprocess.fit(df, column_name=\"name\")\n",
    "        nameprocess.save(path / \"tfidf\")\n",
    "        return \"done\"\n",
    "\n",
    "    else:\n",
    "\n",
    "        # load the required classes \n",
    "        oheobj = OrdinalEncoderTransformer.load()\n",
    "        onehotencoded = oheobj.transform(df, column_name=ordinal_categorical[0])\n",
    "        print(onehotencoded.shape)\n",
    "        print(onehotencoded.columns)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "        nonordinalprocess = NonOrdinalCategoricalTransformer().load()\n",
    "        nonordinalencoded = nonordinalprocess.transform(df, \"neighbourhood_group\")\n",
    "        print(nonordinalencoded.shape)\n",
    "        print(nonordinalencoded.columns)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "        datefeatureprocess = DeltaDatetimeFeature().load()\n",
    "        date_transformed = datefeatureprocess.transform(df, 'last_review')\n",
    "        print(date_transformed.shape)\n",
    "        print(date_transformed.columns)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "        nameprocess = tfidfVectorizer().load()\n",
    "        name_transfored = nameprocess.transform(df, 'name')\n",
    "        print(name_transfored.shape)\n",
    "        print(name_transfored.columns)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        meanimputerprocessed = MeanImputer(mean_imputed_columns)\n",
    "        print(meanimputerprocessed.shape)\n",
    "        print(meanimputerprocessed.columns)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        # datalist = [onehotencoded, nonordinalencoded, date_transformed, name_transfored, meanimputerprocessed]\n",
    "\n",
    "        # final_df = CombineDataFrames(datalist)\n",
    "        # return final_df\n",
    "    \n",
    "        return \"DOne\"\n",
    "\n",
    "\n",
    "\n",
    "def CombineDataFrames(dataframes):\n",
    "    \"\"\" \n",
    "    combine list of dataframes\n",
    "    \"\"\"\n",
    "    result = pd.concat(dataframes, ignore_index=True)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
